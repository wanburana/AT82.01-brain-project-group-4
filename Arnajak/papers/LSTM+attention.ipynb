{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ye-F5QW5BJpG",
    "outputId": "299060ae-33b7-4526-859b-41713425fb6c"
   },
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from glob import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OX1iccZwCGNY",
    "outputId": "06713c82-a7c5-4fe8-fa61-45ce7194d710"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1280, 16, 8064)\n",
      "(1280, 4)\n"
     ]
    }
   ],
   "source": [
    "eeg_emo0 = np.load('Data/eeg_emo_chan.npy')\n",
    "targets = np.load('Data/targets.npy')\n",
    "print(eeg_emo0.shape)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lZLqm3UWCGQl",
    "outputId": "fcf52245-44cf-48e1-94c6-a3cb3ff80a04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1280, 16, 7680)\n"
     ]
    }
   ],
   "source": [
    "eeg_emo = eeg_emo0[:,:,384:] # cut 3s baseline\n",
    "print(eeg_emo.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yephCgGaIOtN"
   },
   "source": [
    "# divided into 12 segments which each is 5 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25SECjfEIUEM",
    "outputId": "afbae610-667c-4afc-e3ac-34f278f8867b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15360, 16, 640)\n",
      "(15360, 4)\n"
     ]
    }
   ],
   "source": [
    "eeg_emo_5s = np.empty((eeg_emo.shape[0]*12,eeg_emo.shape[1],640))\n",
    "targets_5s = np.empty((eeg_emo.shape[0]*12,4))\n",
    "\n",
    "for i,k in enumerate(range(0,eeg_emo_5s.shape[0],12)):\n",
    "    for j in range(12):\n",
    "        eeg_emo_5s[k+j,:,:] = eeg_emo[i,:,640*j:640*(j+1)]\n",
    "        targets_5s[k+j,:] = targets[i,:]\n",
    "print(eeg_emo_5s.shape)\n",
    "print(targets_5s.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssVpSp1SF4fh"
   },
   "source": [
    "# à¸ºValence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tEH_5xUxFxGo",
    "outputId": "b051bd7b-95fe-4905-c3f0-d3f59f423243"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15360,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# valence 0-low ,1 high \n",
    "valence = []\n",
    "mean_valence = np.mean(targets_5s[:,0])\n",
    "for i in range(targets_5s.shape[0]):\n",
    "    if targets_5s[i,0] < mean_valence :\n",
    "        valence.append(0)\n",
    "    else :\n",
    "        valence.append(1)\n",
    "valence = np.array(valence)\n",
    "valence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gkBQJzacCGkB",
    "outputId": "4f8c0db6-5e21-4bd9-c70e-a59fc04d4162"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15360, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "valence = to_categorical(valence)\n",
    "valence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "tMR6eTJECGmZ"
   },
   "outputs": [],
   "source": [
    "train_size = 0.7\n",
    "val_size = 0.1\n",
    "idx = np.arange(0,eeg_emo_5s.shape[0],1)\n",
    "ts = int(train_size*eeg_emo_5s.shape[0]) \n",
    "vs = int(val_size*eeg_emo_5s.shape[0]) \n",
    "idx_train = idx[0:ts]\n",
    "idx_val = idx[ts:ts+vs]\n",
    "idx_test = idx[ts+vs:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ix4oAzcUCGoi",
    "outputId": "7dee3712-2458-43a8-d719-dd07b129f805"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train (10752, 16, 640)\n",
      "x_val (1536, 16, 640)\n",
      "x_test (3072, 16, 640)\n",
      "y_train (10752, 2)\n",
      "y_val (1536, 2)\n",
      "y_test (3072, 2)\n"
     ]
    }
   ],
   "source": [
    "x_train = eeg_emo_5s[idx_train]\n",
    "x_val = eeg_emo_5s[idx_val]\n",
    "x_test = eeg_emo_5s[idx_test]\n",
    "\n",
    "y_train = valence[idx_train]\n",
    "y_val = valence[idx_val]\n",
    "y_test = valence[idx_test]\n",
    "print('x_train',x_train.shape)\n",
    "print('x_val',x_val.shape)\n",
    "print('x_test',x_test.shape)\n",
    "\n",
    "print('y_train',y_train.shape)\n",
    "print('y_val',y_val.shape)\n",
    "print('y_test',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Am09M9NfOhaZ"
   },
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "L3FDuguFCGq1"
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "bIE7dRjrCGxQ"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "batch_size = 512\n",
    "def dataload(X, y):\n",
    "    X_tensor = torch.tensor(X).float()\n",
    "    y_tensor = torch.tensor(y)\n",
    "    dataset_tensor = TensorDataset(X_tensor, y_tensor)\n",
    "    return dataset_tensor\n",
    "    #return DataLoader(dataset_tensor, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_loader = DataLoader(dataload(x_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_loader = DataLoader(dataload(x_val, y_val), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_loader = DataLoader(dataload(x_test, y_test), batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HmQvckurOj7V"
   },
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "8n8OGs5SOkPF"
   },
   "outputs": [],
   "source": [
    "#model hyperparameters\n",
    "hidden_dim1 = 64\n",
    "hidden_dim2= 32 \n",
    "embed_dim = x_train.shape[2]\n",
    "#ed2 = hidden_dim*2\n",
    "output_dim = y_train.shape[1]\n",
    "num_layers = 1\n",
    "bidirectional = True\n",
    "dropout = 0.5\n",
    "\n",
    "#training hyperparameters\n",
    "num_epochs =200\n",
    "lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "J_IjqVibOkRR"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "               \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(16, 32, 4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1))      \n",
    "        self.lstm = nn.LSTM(319, hidden_dim1, num_layers=num_layers,\n",
    "                    bidirectional=bidirectional, \n",
    "                    dropout=dropout,\n",
    "                    batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim1*2, output_dim)   \n",
    "    \n",
    "    \n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "        hidden = final_state.unsqueeze(2)  # hidden : [batch_size, n_hidden * num_directions(=2), 1(=n_layer)]\n",
    "        attn_weights = torch.bmm(lstm_output, hidden).squeeze(2) # attn_weights : [batch_size, seq_len, 1]\n",
    "        soft_attn_weights = F.softmax(attn_weights, 1)\n",
    "        # [batch_size, n_hidden * num_directions(=2), seq_len] * [batch_size, seq_len, 1] = [batch_size, n_hidden * num_directions(=2), 1]\n",
    "        context = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
    "        return context, soft_attn_weights.cpu().data.numpy() # context : [batch_size, n_hidden * num_directions(=2)]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out0 = self.layer1(x)\n",
    "        out,(hn, cn) = self.lstm(out0)\n",
    "        hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1)\n",
    "        \n",
    "        attn_output, attention = self.attention_net(out, hn)\n",
    "        \n",
    "        return self.fc(attn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "8Kl-hCGVQ7V9"
   },
   "outputs": [],
   "source": [
    "#explicitly initialize weights for better learning\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, (nn.Conv2d, nn.Conv2d)):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.kaiming_normal_(param) #<---here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fv5ZjUSdQ9gs",
    "outputId": "24ccc7bf-a6bf-4930-a935-c72256ad74c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv1d(16, 32, kernel_size=(4,), stride=(2,))\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lstm): LSTM(319, 64, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTM().cuda(3)\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "htjSK8grQ9i6"
   },
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmtQFB67HCoR"
   },
   "source": [
    "# Train Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "kI7GHuLLIdyz"
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "H_ZVkAzNId1V"
   },
   "outputs": [],
   "source": [
    "def cnn_acc(preds,y):\n",
    "    _, predicted = torch.max(preds.data, 1)\n",
    "    correct = (predicted == y).sum()\n",
    "    acc = correct / y.size(0)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "r76aiXrnId3k"
   },
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train() #useful for batchnorm and dropout\n",
    "    for i, (features, label) in enumerate(loader): \n",
    "        features   = features.cuda(3)\n",
    "        label = label.cuda(3)\n",
    "        #predict\n",
    "        predictions = model(features)\n",
    "        #calculate loss\n",
    "        loss = criterion(predictions, label)\n",
    "        \"\"\"l2_lambda = 0.001\n",
    "        l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "        loss = loss + l2_lambda * l2_norm\"\"\" \n",
    "        acc = binary_accuracy(predictions, label)\n",
    "        \n",
    "        #backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "                \n",
    "    return epoch_loss / len(loader), epoch_acc / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "Bqj2y7T-Id5d"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (features, label) in enumerate(loader): \n",
    "            features   = features.cuda(3)\n",
    "            label = label.cuda(3)\n",
    "\n",
    "            predictions = model(features)\n",
    "            \n",
    "            loss = criterion(predictions, label)\n",
    "            acc = binary_accuracy(predictions, label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(loader), epoch_acc / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EzIfXUOtId7p",
    "outputId": "e55929f1-91fb-48ef-da06-864ebdb7071e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Train Loss: 0.707 | Train Acc: 50.98%\n",
      "\t Val. Loss: 0.696 |  Val. Acc: 52.64%\n",
      "Epoch: 02 | Train Loss: 0.692 | Train Acc: 53.80%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 52.70%\n",
      "Epoch: 03 | Train Loss: 0.685 | Train Acc: 54.82%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 52.38%\n",
      "Epoch: 04 | Train Loss: 0.679 | Train Acc: 56.22%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 52.41%\n",
      "Epoch: 05 | Train Loss: 0.674 | Train Acc: 58.32%\n",
      "\t Val. Loss: 0.696 |  Val. Acc: 53.61%\n",
      "Epoch: 06 | Train Loss: 0.668 | Train Acc: 58.98%\n",
      "\t Val. Loss: 0.697 |  Val. Acc: 52.34%\n",
      "Epoch: 07 | Train Loss: 0.662 | Train Acc: 60.74%\n",
      "\t Val. Loss: 0.698 |  Val. Acc: 52.18%\n",
      "Epoch: 08 | Train Loss: 0.653 | Train Acc: 62.12%\n",
      "\t Val. Loss: 0.705 |  Val. Acc: 52.93%\n",
      "Epoch: 09 | Train Loss: 0.643 | Train Acc: 63.79%\n",
      "\t Val. Loss: 0.710 |  Val. Acc: 51.99%\n",
      "Epoch: 10 | Train Loss: 0.637 | Train Acc: 64.61%\n",
      "\t Val. Loss: 0.714 |  Val. Acc: 51.11%\n",
      "Epoch: 11 | Train Loss: 0.630 | Train Acc: 65.24%\n",
      "\t Val. Loss: 0.718 |  Val. Acc: 52.31%\n",
      "Epoch: 12 | Train Loss: 0.612 | Train Acc: 67.54%\n",
      "\t Val. Loss: 0.732 |  Val. Acc: 51.69%\n",
      "Epoch: 13 | Train Loss: 0.603 | Train Acc: 68.33%\n",
      "\t Val. Loss: 0.738 |  Val. Acc: 50.49%\n",
      "Epoch: 14 | Train Loss: 0.591 | Train Acc: 69.75%\n",
      "\t Val. Loss: 0.742 |  Val. Acc: 51.82%\n",
      "Epoch: 15 | Train Loss: 0.582 | Train Acc: 70.55%\n",
      "\t Val. Loss: 0.757 |  Val. Acc: 50.91%\n",
      "Epoch: 16 | Train Loss: 0.573 | Train Acc: 71.42%\n",
      "\t Val. Loss: 0.753 |  Val. Acc: 51.86%\n",
      "Epoch: 17 | Train Loss: 0.558 | Train Acc: 72.46%\n",
      "\t Val. Loss: 0.763 |  Val. Acc: 51.24%\n",
      "Epoch: 18 | Train Loss: 0.542 | Train Acc: 74.27%\n",
      "\t Val. Loss: 0.776 |  Val. Acc: 50.55%\n",
      "Epoch: 19 | Train Loss: 0.527 | Train Acc: 75.20%\n",
      "\t Val. Loss: 0.792 |  Val. Acc: 50.23%\n",
      "Epoch: 20 | Train Loss: 0.516 | Train Acc: 76.33%\n",
      "\t Val. Loss: 0.800 |  Val. Acc: 51.27%\n",
      "Epoch: 21 | Train Loss: 0.498 | Train Acc: 77.01%\n",
      "\t Val. Loss: 0.817 |  Val. Acc: 49.97%\n",
      "Epoch: 22 | Train Loss: 0.487 | Train Acc: 78.00%\n",
      "\t Val. Loss: 0.814 |  Val. Acc: 49.28%\n",
      "Epoch: 23 | Train Loss: 0.473 | Train Acc: 78.99%\n",
      "\t Val. Loss: 0.839 |  Val. Acc: 49.64%\n",
      "Epoch: 24 | Train Loss: 0.462 | Train Acc: 79.63%\n",
      "\t Val. Loss: 0.865 |  Val. Acc: 51.69%\n",
      "Epoch: 25 | Train Loss: 0.446 | Train Acc: 80.59%\n",
      "\t Val. Loss: 0.844 |  Val. Acc: 50.55%\n",
      "Epoch: 26 | Train Loss: 0.435 | Train Acc: 81.12%\n",
      "\t Val. Loss: 0.865 |  Val. Acc: 50.78%\n",
      "Epoch: 27 | Train Loss: 0.427 | Train Acc: 81.98%\n",
      "\t Val. Loss: 0.872 |  Val. Acc: 49.74%\n",
      "Epoch: 28 | Train Loss: 0.407 | Train Acc: 83.29%\n",
      "\t Val. Loss: 0.900 |  Val. Acc: 51.01%\n",
      "Epoch: 29 | Train Loss: 0.401 | Train Acc: 82.92%\n",
      "\t Val. Loss: 0.906 |  Val. Acc: 51.95%\n",
      "Epoch: 30 | Train Loss: 0.381 | Train Acc: 84.58%\n",
      "\t Val. Loss: 0.916 |  Val. Acc: 50.81%\n",
      "Epoch: 31 | Train Loss: 0.378 | Train Acc: 84.36%\n",
      "\t Val. Loss: 0.911 |  Val. Acc: 51.40%\n",
      "Epoch: 32 | Train Loss: 0.361 | Train Acc: 85.50%\n",
      "\t Val. Loss: 0.919 |  Val. Acc: 51.63%\n",
      "Epoch: 33 | Train Loss: 0.357 | Train Acc: 85.64%\n",
      "\t Val. Loss: 0.947 |  Val. Acc: 50.55%\n",
      "Epoch: 34 | Train Loss: 0.348 | Train Acc: 86.47%\n",
      "\t Val. Loss: 0.957 |  Val. Acc: 51.01%\n",
      "Epoch: 35 | Train Loss: 0.335 | Train Acc: 86.82%\n",
      "\t Val. Loss: 0.986 |  Val. Acc: 50.65%\n",
      "Epoch: 36 | Train Loss: 0.326 | Train Acc: 87.51%\n",
      "\t Val. Loss: 0.948 |  Val. Acc: 51.37%\n",
      "Epoch: 37 | Train Loss: 0.312 | Train Acc: 88.63%\n",
      "\t Val. Loss: 0.985 |  Val. Acc: 50.94%\n",
      "Epoch: 38 | Train Loss: 0.305 | Train Acc: 88.63%\n",
      "\t Val. Loss: 1.003 |  Val. Acc: 51.56%\n",
      "Epoch: 39 | Train Loss: 0.300 | Train Acc: 88.51%\n",
      "\t Val. Loss: 1.020 |  Val. Acc: 52.64%\n",
      "Epoch: 40 | Train Loss: 0.292 | Train Acc: 89.26%\n",
      "\t Val. Loss: 1.000 |  Val. Acc: 51.79%\n",
      "Epoch: 41 | Train Loss: 0.273 | Train Acc: 89.92%\n",
      "\t Val. Loss: 1.038 |  Val. Acc: 52.67%\n",
      "Epoch: 42 | Train Loss: 0.265 | Train Acc: 90.29%\n",
      "\t Val. Loss: 1.002 |  Val. Acc: 52.99%\n",
      "Epoch: 43 | Train Loss: 0.261 | Train Acc: 90.81%\n",
      "\t Val. Loss: 1.046 |  Val. Acc: 52.41%\n",
      "Epoch: 44 | Train Loss: 0.256 | Train Acc: 90.48%\n",
      "\t Val. Loss: 1.063 |  Val. Acc: 51.17%\n",
      "Epoch: 45 | Train Loss: 0.256 | Train Acc: 90.97%\n",
      "\t Val. Loss: 1.042 |  Val. Acc: 50.55%\n",
      "Epoch: 46 | Train Loss: 0.243 | Train Acc: 91.27%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 51.24%\n",
      "Epoch: 47 | Train Loss: 0.238 | Train Acc: 91.32%\n",
      "\t Val. Loss: 1.081 |  Val. Acc: 52.96%\n",
      "Epoch: 48 | Train Loss: 0.229 | Train Acc: 91.70%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 51.20%\n",
      "Epoch: 49 | Train Loss: 0.220 | Train Acc: 92.61%\n",
      "\t Val. Loss: 1.145 |  Val. Acc: 49.41%\n",
      "Epoch: 50 | Train Loss: 0.223 | Train Acc: 92.05%\n",
      "\t Val. Loss: 1.108 |  Val. Acc: 52.93%\n",
      "Epoch: 51 | Train Loss: 0.227 | Train Acc: 91.96%\n",
      "\t Val. Loss: 1.173 |  Val. Acc: 50.42%\n",
      "Epoch: 52 | Train Loss: 0.219 | Train Acc: 92.45%\n",
      "\t Val. Loss: 1.189 |  Val. Acc: 50.20%\n",
      "Epoch: 53 | Train Loss: 0.207 | Train Acc: 92.95%\n",
      "\t Val. Loss: 1.180 |  Val. Acc: 51.01%\n",
      "Epoch: 54 | Train Loss: 0.213 | Train Acc: 92.47%\n",
      "\t Val. Loss: 1.152 |  Val. Acc: 51.79%\n",
      "Epoch: 55 | Train Loss: 0.204 | Train Acc: 93.13%\n",
      "\t Val. Loss: 1.166 |  Val. Acc: 50.26%\n",
      "Epoch: 56 | Train Loss: 0.202 | Train Acc: 93.00%\n",
      "\t Val. Loss: 1.225 |  Val. Acc: 50.26%\n",
      "Epoch: 57 | Train Loss: 0.195 | Train Acc: 93.43%\n",
      "\t Val. Loss: 1.179 |  Val. Acc: 52.05%\n",
      "Epoch: 58 | Train Loss: 0.190 | Train Acc: 93.61%\n",
      "\t Val. Loss: 1.177 |  Val. Acc: 52.15%\n",
      "Epoch: 59 | Train Loss: 0.184 | Train Acc: 93.88%\n",
      "\t Val. Loss: 1.188 |  Val. Acc: 52.05%\n",
      "Epoch: 60 | Train Loss: 0.182 | Train Acc: 93.95%\n",
      "\t Val. Loss: 1.215 |  Val. Acc: 51.89%\n",
      "Epoch: 61 | Train Loss: 0.180 | Train Acc: 94.01%\n",
      "\t Val. Loss: 1.196 |  Val. Acc: 53.48%\n",
      "Epoch: 62 | Train Loss: 0.172 | Train Acc: 94.55%\n",
      "\t Val. Loss: 1.220 |  Val. Acc: 50.91%\n",
      "Epoch: 63 | Train Loss: 0.167 | Train Acc: 94.75%\n",
      "\t Val. Loss: 1.225 |  Val. Acc: 50.88%\n",
      "Epoch: 64 | Train Loss: 0.162 | Train Acc: 94.94%\n",
      "\t Val. Loss: 1.290 |  Val. Acc: 50.52%\n",
      "Epoch: 65 | Train Loss: 0.157 | Train Acc: 94.92%\n",
      "\t Val. Loss: 1.319 |  Val. Acc: 50.00%\n",
      "Epoch: 66 | Train Loss: 0.154 | Train Acc: 95.20%\n",
      "\t Val. Loss: 1.289 |  Val. Acc: 50.29%\n",
      "Epoch: 67 | Train Loss: 0.152 | Train Acc: 95.37%\n",
      "\t Val. Loss: 1.285 |  Val. Acc: 52.25%\n",
      "Epoch: 68 | Train Loss: 0.158 | Train Acc: 95.01%\n",
      "\t Val. Loss: 1.344 |  Val. Acc: 51.27%\n",
      "Epoch: 69 | Train Loss: 0.154 | Train Acc: 95.23%\n",
      "\t Val. Loss: 1.304 |  Val. Acc: 52.34%\n",
      "Epoch: 70 | Train Loss: 0.142 | Train Acc: 95.56%\n",
      "\t Val. Loss: 1.320 |  Val. Acc: 52.64%\n",
      "Epoch: 71 | Train Loss: 0.150 | Train Acc: 95.06%\n",
      "\t Val. Loss: 1.286 |  Val. Acc: 51.27%\n",
      "Epoch: 72 | Train Loss: 0.147 | Train Acc: 95.52%\n",
      "\t Val. Loss: 1.327 |  Val. Acc: 50.98%\n",
      "Epoch: 73 | Train Loss: 0.141 | Train Acc: 95.32%\n",
      "\t Val. Loss: 1.315 |  Val. Acc: 51.30%\n",
      "Epoch: 74 | Train Loss: 0.145 | Train Acc: 95.48%\n",
      "\t Val. Loss: 1.301 |  Val. Acc: 52.96%\n",
      "Epoch: 75 | Train Loss: 0.138 | Train Acc: 95.80%\n",
      "\t Val. Loss: 1.315 |  Val. Acc: 52.99%\n",
      "Epoch: 76 | Train Loss: 0.136 | Train Acc: 95.82%\n",
      "\t Val. Loss: 1.352 |  Val. Acc: 50.98%\n",
      "Epoch: 77 | Train Loss: 0.131 | Train Acc: 95.81%\n",
      "\t Val. Loss: 1.386 |  Val. Acc: 50.42%\n",
      "Epoch: 78 | Train Loss: 0.133 | Train Acc: 95.68%\n",
      "\t Val. Loss: 1.365 |  Val. Acc: 49.54%\n",
      "Epoch: 79 | Train Loss: 0.131 | Train Acc: 96.08%\n",
      "\t Val. Loss: 1.350 |  Val. Acc: 51.07%\n",
      "Epoch: 80 | Train Loss: 0.128 | Train Acc: 95.90%\n",
      "\t Val. Loss: 1.418 |  Val. Acc: 51.17%\n",
      "Epoch: 81 | Train Loss: 0.133 | Train Acc: 95.86%\n",
      "\t Val. Loss: 1.405 |  Val. Acc: 49.90%\n",
      "Epoch: 82 | Train Loss: 0.132 | Train Acc: 95.59%\n",
      "\t Val. Loss: 1.425 |  Val. Acc: 51.50%\n",
      "Epoch: 83 | Train Loss: 0.133 | Train Acc: 95.90%\n",
      "\t Val. Loss: 1.355 |  Val. Acc: 51.73%\n",
      "Epoch: 84 | Train Loss: 0.124 | Train Acc: 96.15%\n",
      "\t Val. Loss: 1.452 |  Val. Acc: 50.75%\n",
      "Epoch: 85 | Train Loss: 0.126 | Train Acc: 95.84%\n",
      "\t Val. Loss: 1.423 |  Val. Acc: 52.15%\n",
      "Epoch: 86 | Train Loss: 0.120 | Train Acc: 96.38%\n",
      "\t Val. Loss: 1.365 |  Val. Acc: 51.43%\n",
      "Epoch: 87 | Train Loss: 0.122 | Train Acc: 96.16%\n",
      "\t Val. Loss: 1.389 |  Val. Acc: 51.46%\n",
      "Epoch: 88 | Train Loss: 0.125 | Train Acc: 95.66%\n",
      "\t Val. Loss: 1.503 |  Val. Acc: 49.80%\n",
      "Epoch: 89 | Train Loss: 0.121 | Train Acc: 96.07%\n",
      "\t Val. Loss: 1.471 |  Val. Acc: 48.40%\n",
      "Epoch: 90 | Train Loss: 0.116 | Train Acc: 96.29%\n",
      "\t Val. Loss: 1.456 |  Val. Acc: 50.85%\n",
      "Epoch: 91 | Train Loss: 0.118 | Train Acc: 96.03%\n",
      "\t Val. Loss: 1.404 |  Val. Acc: 51.43%\n",
      "Epoch: 92 | Train Loss: 0.120 | Train Acc: 96.27%\n",
      "\t Val. Loss: 1.424 |  Val. Acc: 51.69%\n",
      "Epoch: 93 | Train Loss: 0.118 | Train Acc: 96.30%\n",
      "\t Val. Loss: 1.444 |  Val. Acc: 52.12%\n",
      "Epoch: 94 | Train Loss: 0.113 | Train Acc: 96.40%\n",
      "\t Val. Loss: 1.487 |  Val. Acc: 51.50%\n",
      "Epoch: 95 | Train Loss: 0.116 | Train Acc: 96.01%\n",
      "\t Val. Loss: 1.494 |  Val. Acc: 50.16%\n",
      "Epoch: 96 | Train Loss: 0.112 | Train Acc: 96.24%\n",
      "\t Val. Loss: 1.540 |  Val. Acc: 49.45%\n",
      "Epoch: 97 | Train Loss: 0.106 | Train Acc: 96.59%\n",
      "\t Val. Loss: 1.425 |  Val. Acc: 50.98%\n",
      "Epoch: 98 | Train Loss: 0.108 | Train Acc: 96.57%\n",
      "\t Val. Loss: 1.463 |  Val. Acc: 51.95%\n",
      "Epoch: 99 | Train Loss: 0.110 | Train Acc: 96.34%\n",
      "\t Val. Loss: 1.480 |  Val. Acc: 51.56%\n",
      "Epoch: 100 | Train Loss: 0.112 | Train Acc: 96.52%\n",
      "\t Val. Loss: 1.491 |  Val. Acc: 53.16%\n",
      "Epoch: 101 | Train Loss: 0.111 | Train Acc: 96.36%\n",
      "\t Val. Loss: 1.480 |  Val. Acc: 52.93%\n",
      "Epoch: 102 | Train Loss: 0.103 | Train Acc: 96.70%\n",
      "\t Val. Loss: 1.502 |  Val. Acc: 51.30%\n",
      "Epoch: 103 | Train Loss: 0.105 | Train Acc: 96.67%\n",
      "\t Val. Loss: 1.490 |  Val. Acc: 50.85%\n",
      "Epoch: 104 | Train Loss: 0.097 | Train Acc: 97.14%\n",
      "\t Val. Loss: 1.529 |  Val. Acc: 50.98%\n",
      "Epoch: 105 | Train Loss: 0.096 | Train Acc: 97.09%\n",
      "\t Val. Loss: 1.477 |  Val. Acc: 52.12%\n",
      "Epoch: 106 | Train Loss: 0.101 | Train Acc: 96.68%\n",
      "\t Val. Loss: 1.491 |  Val. Acc: 51.60%\n",
      "Epoch: 107 | Train Loss: 0.097 | Train Acc: 97.03%\n",
      "\t Val. Loss: 1.533 |  Val. Acc: 49.64%\n",
      "Epoch: 108 | Train Loss: 0.097 | Train Acc: 97.12%\n",
      "\t Val. Loss: 1.520 |  Val. Acc: 52.38%\n",
      "Epoch: 109 | Train Loss: 0.092 | Train Acc: 97.03%\n",
      "\t Val. Loss: 1.498 |  Val. Acc: 52.73%\n",
      "Epoch: 110 | Train Loss: 0.094 | Train Acc: 97.09%\n",
      "\t Val. Loss: 1.494 |  Val. Acc: 50.78%\n",
      "Epoch: 111 | Train Loss: 0.098 | Train Acc: 97.06%\n",
      "\t Val. Loss: 1.526 |  Val. Acc: 50.55%\n",
      "Epoch: 112 | Train Loss: 0.092 | Train Acc: 97.10%\n",
      "\t Val. Loss: 1.526 |  Val. Acc: 51.30%\n",
      "Epoch: 113 | Train Loss: 0.099 | Train Acc: 96.79%\n",
      "\t Val. Loss: 1.483 |  Val. Acc: 51.76%\n",
      "Epoch: 114 | Train Loss: 0.090 | Train Acc: 97.34%\n",
      "\t Val. Loss: 1.559 |  Val. Acc: 51.17%\n",
      "Epoch: 115 | Train Loss: 0.089 | Train Acc: 97.26%\n",
      "\t Val. Loss: 1.529 |  Val. Acc: 51.37%\n",
      "Epoch: 116 | Train Loss: 0.085 | Train Acc: 97.33%\n",
      "\t Val. Loss: 1.559 |  Val. Acc: 50.36%\n",
      "Epoch: 117 | Train Loss: 0.092 | Train Acc: 97.13%\n",
      "\t Val. Loss: 1.554 |  Val. Acc: 51.17%\n",
      "Epoch: 118 | Train Loss: 0.088 | Train Acc: 97.31%\n",
      "\t Val. Loss: 1.579 |  Val. Acc: 48.80%\n",
      "Epoch: 119 | Train Loss: 0.091 | Train Acc: 97.04%\n",
      "\t Val. Loss: 1.543 |  Val. Acc: 50.10%\n",
      "Epoch: 120 | Train Loss: 0.093 | Train Acc: 97.21%\n",
      "\t Val. Loss: 1.562 |  Val. Acc: 50.75%\n",
      "Epoch: 121 | Train Loss: 0.088 | Train Acc: 97.29%\n",
      "\t Val. Loss: 1.564 |  Val. Acc: 51.63%\n",
      "Epoch: 122 | Train Loss: 0.096 | Train Acc: 97.10%\n",
      "\t Val. Loss: 1.544 |  Val. Acc: 50.85%\n",
      "Epoch: 123 | Train Loss: 0.099 | Train Acc: 96.65%\n",
      "\t Val. Loss: 1.545 |  Val. Acc: 51.99%\n",
      "Epoch: 124 | Train Loss: 0.097 | Train Acc: 96.86%\n",
      "\t Val. Loss: 1.512 |  Val. Acc: 52.86%\n",
      "Epoch: 125 | Train Loss: 0.089 | Train Acc: 97.34%\n",
      "\t Val. Loss: 1.623 |  Val. Acc: 51.46%\n",
      "Epoch: 126 | Train Loss: 0.084 | Train Acc: 97.35%\n",
      "\t Val. Loss: 1.567 |  Val. Acc: 51.89%\n",
      "Epoch: 127 | Train Loss: 0.086 | Train Acc: 97.15%\n",
      "\t Val. Loss: 1.612 |  Val. Acc: 52.05%\n",
      "Epoch: 128 | Train Loss: 0.088 | Train Acc: 97.21%\n",
      "\t Val. Loss: 1.589 |  Val. Acc: 51.53%\n",
      "Epoch: 129 | Train Loss: 0.089 | Train Acc: 97.01%\n",
      "\t Val. Loss: 1.629 |  Val. Acc: 51.17%\n",
      "Epoch: 130 | Train Loss: 0.080 | Train Acc: 97.45%\n",
      "\t Val. Loss: 1.634 |  Val. Acc: 50.29%\n",
      "Epoch: 131 | Train Loss: 0.084 | Train Acc: 97.20%\n",
      "\t Val. Loss: 1.602 |  Val. Acc: 51.99%\n",
      "Epoch: 132 | Train Loss: 0.085 | Train Acc: 97.33%\n",
      "\t Val. Loss: 1.584 |  Val. Acc: 50.39%\n",
      "Epoch: 133 | Train Loss: 0.080 | Train Acc: 97.41%\n",
      "\t Val. Loss: 1.518 |  Val. Acc: 51.79%\n",
      "Epoch: 134 | Train Loss: 0.093 | Train Acc: 96.90%\n",
      "\t Val. Loss: 1.563 |  Val. Acc: 51.63%\n",
      "Epoch: 135 | Train Loss: 0.088 | Train Acc: 97.26%\n",
      "\t Val. Loss: 1.576 |  Val. Acc: 50.75%\n",
      "Epoch: 136 | Train Loss: 0.075 | Train Acc: 97.65%\n",
      "\t Val. Loss: 1.590 |  Val. Acc: 51.30%\n",
      "Epoch: 137 | Train Loss: 0.077 | Train Acc: 97.67%\n",
      "\t Val. Loss: 1.630 |  Val. Acc: 49.25%\n",
      "Epoch: 138 | Train Loss: 0.072 | Train Acc: 97.93%\n",
      "\t Val. Loss: 1.672 |  Val. Acc: 51.11%\n",
      "Epoch: 139 | Train Loss: 0.080 | Train Acc: 97.48%\n",
      "\t Val. Loss: 1.646 |  Val. Acc: 50.81%\n",
      "Epoch: 140 | Train Loss: 0.075 | Train Acc: 97.65%\n",
      "\t Val. Loss: 1.658 |  Val. Acc: 51.04%\n",
      "Epoch: 141 | Train Loss: 0.083 | Train Acc: 97.42%\n",
      "\t Val. Loss: 1.690 |  Val. Acc: 50.00%\n",
      "Epoch: 142 | Train Loss: 0.072 | Train Acc: 97.85%\n",
      "\t Val. Loss: 1.693 |  Val. Acc: 50.20%\n",
      "Epoch: 143 | Train Loss: 0.077 | Train Acc: 97.74%\n",
      "\t Val. Loss: 1.640 |  Val. Acc: 50.20%\n",
      "Epoch: 144 | Train Loss: 0.077 | Train Acc: 97.60%\n",
      "\t Val. Loss: 1.695 |  Val. Acc: 50.62%\n",
      "Epoch: 145 | Train Loss: 0.072 | Train Acc: 97.63%\n",
      "\t Val. Loss: 1.718 |  Val. Acc: 49.80%\n",
      "Epoch: 146 | Train Loss: 0.081 | Train Acc: 97.41%\n",
      "\t Val. Loss: 1.649 |  Val. Acc: 51.56%\n",
      "Epoch: 147 | Train Loss: 0.078 | Train Acc: 97.58%\n",
      "\t Val. Loss: 1.685 |  Val. Acc: 49.51%\n",
      "Epoch: 148 | Train Loss: 0.067 | Train Acc: 97.93%\n",
      "\t Val. Loss: 1.659 |  Val. Acc: 50.33%\n",
      "Epoch: 149 | Train Loss: 0.070 | Train Acc: 97.93%\n",
      "\t Val. Loss: 1.673 |  Val. Acc: 50.72%\n",
      "Epoch: 150 | Train Loss: 0.076 | Train Acc: 97.58%\n",
      "\t Val. Loss: 1.680 |  Val. Acc: 50.75%\n",
      "Epoch: 151 | Train Loss: 0.072 | Train Acc: 97.67%\n",
      "\t Val. Loss: 1.668 |  Val. Acc: 50.75%\n",
      "Epoch: 152 | Train Loss: 0.075 | Train Acc: 97.63%\n",
      "\t Val. Loss: 1.669 |  Val. Acc: 50.55%\n",
      "Epoch: 153 | Train Loss: 0.076 | Train Acc: 97.64%\n",
      "\t Val. Loss: 1.763 |  Val. Acc: 49.93%\n",
      "Epoch: 154 | Train Loss: 0.069 | Train Acc: 97.74%\n",
      "\t Val. Loss: 1.680 |  Val. Acc: 50.78%\n",
      "Epoch: 155 | Train Loss: 0.073 | Train Acc: 97.67%\n",
      "\t Val. Loss: 1.735 |  Val. Acc: 51.50%\n",
      "Epoch: 156 | Train Loss: 0.070 | Train Acc: 97.81%\n",
      "\t Val. Loss: 1.757 |  Val. Acc: 50.94%\n",
      "Epoch: 157 | Train Loss: 0.065 | Train Acc: 98.14%\n",
      "\t Val. Loss: 1.803 |  Val. Acc: 50.72%\n",
      "Epoch: 158 | Train Loss: 0.062 | Train Acc: 98.12%\n",
      "\t Val. Loss: 1.821 |  Val. Acc: 49.90%\n",
      "Epoch: 159 | Train Loss: 0.068 | Train Acc: 98.00%\n",
      "\t Val. Loss: 1.690 |  Val. Acc: 51.14%\n",
      "Epoch: 160 | Train Loss: 0.066 | Train Acc: 97.87%\n",
      "\t Val. Loss: 1.763 |  Val. Acc: 49.38%\n",
      "Epoch: 161 | Train Loss: 0.070 | Train Acc: 97.71%\n",
      "\t Val. Loss: 1.675 |  Val. Acc: 50.20%\n",
      "Epoch: 162 | Train Loss: 0.064 | Train Acc: 98.02%\n",
      "\t Val. Loss: 1.746 |  Val. Acc: 49.32%\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "valid_losses = []\n",
    "valid_accs = []\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    train_loss,train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    valid_loss,valid_acc = evaluate(model, valid_loader, criterion)\n",
    "    \n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    valid_losses.append(valid_loss)\n",
    "    valid_accs.append(valid_acc)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(),'tensor.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405
    },
    "id": "sYoo5fXpHEG1",
    "outputId": "7948e430-6105-40b3-bf60-dbccc9be882f"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405
    },
    "id": "mOHzYQU2HO5v",
    "outputId": "bb35dd18-398a-4baa-c2c6-813307982b15"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_accs, label = 'train acc')\n",
    "ax.plot(valid_accs, label = 'valid acc')\n",
    "plt.legend()\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VmAk-Nr-HQEU",
    "outputId": "07450d4d-3a2c-4b26-e403-d1a9b4ef439c"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('tensor.pt'))\n",
    "test_loss , test_acc = evaluate(model, test_loader, criterion)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JNTmTU0lQ9nB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "LSTM.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
